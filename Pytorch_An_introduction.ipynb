{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNufaXoARwHMa8usmSFBPZO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/franciskyalo/Pytorch-Learning/blob/main/Pytorch_An_introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# An Introduction to Pytorch\n"
      ],
      "metadata": {
        "id": "OoxR__sO_eez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch is an open-source machine learning framework primarily used for deep learning applications. It has gained popularity for its flexibility, dynamic computation graph, and user-friendly interface. Here's a brief history of PyTorch:\n",
        "\n",
        "### Origins\n",
        "- **2016**: PyTorch was developed by the **Artificial Intelligence Research Group (FAIR)** at Facebook. It was built on the foundation of **Torch**, a machine learning library written in Lua, which had been widely used in academic research but lacked adoption in the broader industry.\n",
        "\n",
        "- **Key Contributors**:\n",
        "  - **Adam Paszke** and other researchers from FAIR played significant roles in its development.\n",
        "  - It incorporated many modern design principles to make it more accessible and flexible compared to Torch.\n",
        "\n",
        "### Early Features\n",
        "- PyTorch introduced a **dynamic computation graph**, allowing developers to modify the model architecture during runtime. This was a significant departure from static graph frameworks like TensorFlow 1.x, making it especially appealing to researchers.\n",
        "\n",
        "- PyTorch was designed with a focus on Python, making it intuitive and easy to integrate with Pythonâ€™s rich ecosystem of libraries.\n",
        "\n",
        "### Adoption and Growth\n",
        "- By **2017**, PyTorch began gaining traction among researchers, thanks to its ease of use, dynamic nature, and robust community support.\n",
        "\n",
        "- It became a favorite in academic research, where experimentation and model iteration are frequent.\n",
        "\n",
        "### Key Milestones\n",
        "1. **2018**:\n",
        "   - **PyTorch 1.0**: Facebook merged PyTorch with another of its frameworks, **Caffe2**, to combine research flexibility with production capabilities.\n",
        "   - This version included improved support for deploying models in production environments.\n",
        "\n",
        "2. **2019**:\n",
        "   - PyTorch introduced **TorchScript**, enabling the export of models to a statically defined computational graph for optimized deployment.\n",
        "   - It became the framework of choice for many prestigious conferences and competitions in the deep learning community.\n",
        "\n",
        "3. **2020**:\n",
        "   - PyTorch released support for **distributed training** and **hardware accelerators** like TPUs.\n",
        "   - It was increasingly adopted by industry leaders like Tesla, Microsoft, and Uber for real-world applications.\n",
        "\n",
        "4. **2021-2023**:\n",
        "   - PyTorch grew its ecosystem with libraries like **TorchVision** (for computer vision), **TorchText** (for natural language processing), and **TorchAudio** (for audio processing).\n",
        "   - It introduced advanced features like **torch.compile** (dynamic compilation for performance optimization).\n",
        "\n",
        "5. **2022**:\n",
        "   - Facebook rebranded as Meta, reaffirming its commitment to PyTorch by transferring the framework to the **PyTorch Foundation**, part of the Linux Foundation. This ensured that PyTorch would remain community-driven and open-source.\n",
        "\n",
        "6. **2023 and Beyond**:\n",
        "   - PyTorch continues to innovate with improvements in scalability, support for large models (e.g., Transformers), and integration with tools for reinforcement learning, generative AI, and more.\n",
        "\n",
        "### Impact\n",
        "- PyTorch has played a crucial role in democratizing deep learning by providing an accessible yet powerful platform for both researchers and practitioners.\n",
        "- It remains one of the most widely used frameworks in academia and industry, competing closely with TensorFlow.\n",
        "\n",
        "PyTorch's emphasis on flexibility, coupled with its growing ecosystem and robust community support, has solidified its position as a cornerstone of modern AI and deep learning."
      ],
      "metadata": {
        "id": "irw06-O6BFG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Introduction to Tensors"
      ],
      "metadata": {
        "id": "VE_qCy7LB9j1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **tensor** is a multi-dimensional array or matrix used to represent data in deep learning and other numerical computations. Tensors generalize vectors and matrices to higher dimensions, making them highly versatile for handling complex datasets and computations in neural networks.\n",
        "\n",
        "### Key Characteristics of Tensors\n",
        "1. **Dimensionality**:\n",
        "   - A **scalar** is a tensor with zero dimensions (e.g., \\(5\\)).\n",
        "   - A **vector** is a 1-dimensional tensor (e.g., \\([1, 2, 3]\\)).\n",
        "   - A **matrix** is a 2-dimensional tensor (e.g., \\(\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\)).\n",
        "   - Higher-dimensional tensors are often referred to as n-dimensional tensors.\n",
        "\n",
        "2. **Shape**:\n",
        "   The shape of a tensor indicates the number of elements along each dimension. For example:\n",
        "   - A 1D tensor with 3 elements: `(3,)`.\n",
        "   - A 2D tensor with 2 rows and 3 columns: `(2, 3)`.\n",
        "   - A 3D tensor, e.g., representing a batch of images: `(batch_size, height, width)`.\n",
        "\n",
        "3. **Data Types**:\n",
        "   Tensors can hold various data types like integers, floating-point numbers, or even Boolean values. The data type must be specified when defining the tensor.\n",
        "\n",
        "4. **Operations**:\n",
        "   Tensors are the building blocks of deep learning models. They support various operations such as addition, multiplication, reshaping, slicing, and more. These operations are highly optimized for GPUs and other accelerators.\n",
        "\n",
        "### Importance in Deep Learning\n",
        "- **Data Representation**:\n",
        "  Tensors are used to represent all forms of input data (e.g., images, text, audio).\n",
        "  - Images: Stored as 3D tensors (`[height, width, channels]`).\n",
        "  - Text: Represented as sequences or embeddings in tensors.\n",
        "  - Video: Stored as 4D tensors (`[frames, height, width, channels]`).\n",
        "\n",
        "- **Model Parameters**:\n",
        "  The weights and biases in neural networks are stored as tensors.\n",
        "\n",
        "- **Parallel Computation**:\n",
        "  Frameworks like PyTorch and TensorFlow use tensors to perform computations efficiently on GPUs and TPUs.\n",
        "\n",
        "### Example in PyTorch\n",
        "```python\n",
        "import torch\n",
        "\n",
        "# Scalar\n",
        "scalar = torch.tensor(5)\n",
        "print(scalar.shape)  # Output: torch.Size([])\n",
        "\n",
        "# Vector\n",
        "vector = torch.tensor([1, 2, 3])\n",
        "print(vector.shape)  # Output: torch.Size([3])\n",
        "\n",
        "# Matrix\n",
        "matrix = torch.tensor([[1, 2], [3, 4]])\n",
        "print(matrix.shape)  # Output: torch.Size([2, 2])\n",
        "\n",
        "# 3D Tensor\n",
        "tensor_3d = torch.randn(3, 3, 3)  # Random 3x3x3 tensor\n",
        "print(tensor_3d.shape)  # Output: torch.Size([3, 3, 3])\n",
        "```\n",
        "\n",
        "Tensors are foundational in deep learning, enabling the storage and manipulation of data and parameters in neural networks."
      ],
      "metadata": {
        "id": "mdCmujNG__gC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Writing Pytorch Code"
      ],
      "metadata": {
        "id": "h-Mh25b9H2do"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UgEEPd6BaN4B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb87f8cb-6651-46f7-b375-44239143a639"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "z= torch.zeros(4,3) # creating a tensor with 4 rows and 3 columns\n",
        "\n",
        "print(z)"
      ],
      "metadata": {
        "id": "EiyzBwHrbuyt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77ea12c4-eb2f-4928-ec5e-275311459c1a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(z.dtype) # cheking the data type"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBgokDj8_zcU",
        "outputId": "5c122cc1-aa96-48d1-d88a-ebf88ae464eb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#resnet 142 which has 142 with hidden layers for computer vision"
      ],
      "metadata": {
        "id": "iwDobDOzI0U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output layer and some of the amazing things that happen in deep neural networks"
      ],
      "metadata": {
        "id": "t6b60pvxJl4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Graphic Processing Unit and Tensor Processing Unit"
      ],
      "metadata": {
        "id": "5XlA1fv8Jvyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y7AxqrSO-l8U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}